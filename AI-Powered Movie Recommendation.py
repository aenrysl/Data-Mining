# -*- coding: utf-8 -*-
"""MovieRecommendationSystem(full code).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VoFzoMKIV5oOCpcwdbXyaTFmJHcVITYb
"""

!pip install xgboost
!pip install tensorflow
!pip install lightgbm

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder, StandardScaler,  PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.impute import SimpleImputer
from scipy.stats import chi2_contingency, f_oneway
from scipy.stats import pearsonr, zscore, skew
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, RandomizedSearchCV, KFold
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
import xgboost as xgb
from xgboost import XGBRegressor
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from sklearn.neural_network import MLPRegressor
from sklearn.inspection import permutation_importance
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.dummy import DummyRegressor
from sklearn.compose import ColumnTransformer
from scipy.stats import skew
import lightgbm as lgb
from lightgbm import LGBMRegressor

df = pd.read_csv('MovieRecSys.csv', encoding='latin-1')

print(df)

# Convert 'IMDb' column to numerical and remove '/10'
df['IMDb'] = df['IMDb'].astype(str).str.rstrip('/10')  # Convert to string first
df['IMDb'] = pd.to_numeric(df['IMDb'], errors='coerce')

print(df)

# Check for missing values before removing unnamed columns
print("Missing values before removing unnamed columns:\n", df.isnull().sum())

# Count the number of unique languages
num_languages = df['Language'].nunique()
# Print the result
print(f"There are {num_languages} unique languages in the dataset.")

# Get unique languages
unique_languages = df['Language'].unique() # Gets the unique languages in the 'Language' column
# Print the unique languages
print("Unique Languages in the Dataset:")

# Prints a header
for language in unique_languages: # Iterates through each unique language
    print(language)# Prints the current language

# Find rows with empty 'Language'
empty_language_rows = df[df['Language'].isnull()]
# Print the empty rows
print("Rows with empty 'Language':") # Prints a header
print(empty_language_rows) # Prints the rows with missing 'Language' values

# Create a dictionary mapping movie titles to languages
movie_language_mapping = {
    '7 Khoon Maaf': 'Hindi',
    '7 años': 'Spanish',
    'Backcountry': 'English',
    'Black Is Beltza': 'Spanish',
    'Blue Mountain State: The Rise of Thadland': 'English',
    'Brotherly Love': 'English',
    'Captain Underpants: The First Epic Movie': 'English',
    'Chicken Run': 'English',
    'Clash': 'Arabic',
    'Clown': 'English',
    'Dheepan': 'Tamil',
    'Ella Enchanted': 'English',
    'Hands of Stone': 'English',
    'Kubo and the Two Strings': 'English',
    'Kung Fu Panda': 'English',
    'Making The Witcher': 'English',
    'Nowitzki: The Perfect Shot': 'English',
    'Serena': 'English',
    'Sing': 'English',
    'Smurfs: The Lost Village': 'English',
    'Tangerine': 'English',
    'The Babadook': 'English',
    'The Boss Baby': 'English',
    'The Emoji Movie': 'English',
    'The Lady in Dignity': 'Korean',
    'The Little Rascals': 'English',
    'The Look of Silence': 'Indonesian',
    'The Negotiator': 'English',
    'The Prince of Egypt': 'English',
    'The Wailing': 'Korean',
    'The Wave': 'Norwegian',
    'Trolls': 'English',
    'Veve': 'English',
    'Viral': 'English'
}

# Impute the languages using the mapping
for movie, language in movie_language_mapping.items(): # Iterates through the movie-language mapping
    df.loc[df['Title Name'] == movie, 'Language'] = language # Updates the 'Language' column for matching movie titles

# Print the updated DataFrame to verify the changes
print(df[df['Title Name'].isin(movie_language_mapping.keys())]) # Prints rows where 'Title Name' is in the mapping keys

# Fill missing 'Language' values with 'English'
df['Language'].fillna('English', inplace=True)

# Check for missing values in the entire DataFrame
missing_values = df.isnull().sum()
# Print the results
print("Missing values in the DataFrame:\n", missing_values)

# Check for missing values in the 'Released' column
missing_released = df['Released'].isnull().sum()
# Print the result
print(f"Number of missing values in 'Released': {missing_released}")

# Get the rows with missing values in 'Released'
missing_released_rows = df[df['Released'].isnull()]
# Print the rows
print("Rows with missing 'Released' values:") # Prints a header
print(missing_released_rows) # Prints the rows with missing 'Released' values

# Create a dictionary mapping movie titles to release years
movie_release_mapping = {
    '30 for 30: Rand University': 2014,
    '30 for 30: The U': 2009,
    'About Love and Passion': 2006,
    'African Folktales Reimagined': 2023,
    'All Eyes on Him': 2021,
    'Badrinath Ki Dulhania': 2017,
    'Bebefinn': 2022,
    'Best of Stand-Up 2022': 2022,
    'Bhola Shankar (Tamil)': 2023,
    'Chief Daddy 2 - Going for Broke': 2022,
    'El límite infinito': 2019,
    'Friday Night Plan': 2023,
    'Gen Hoshino Concert Recollections 2015-2023': 2023,
    'Ijogbon': 2023,
    'Inkabi': 2024,
    'Jagun Jagun': 2023,
    'Jakarta vs Everybody': 2021,
    'Jumping from High Places': 2022,
    'Kabuki Akadousuzunosuke': 2022,
    'Katt Williams: World War III': 2022,
    'LOL Surprise! Winter Fashion Show': 2022,
    'Leligar': 2021,
    'LiSA LiVE is Smile Always, Eve&Birth: The Birth at Nippon Budokab': 2022,
    'Lolo and the Kid': 2024,
    'My Brother is up on the Tree': 2023,
    'Neal Brennan: Crazy Good': 2024,
    'Pete Holmes: I Am Not for Everyone': 2023,
    'Soldiers in the Camp': 2003,
    'Spookley and the Christmas Kittens': 2019,
    'Three Widows Against The World': 2022,
    'To Love Is To Grow': 2023,
    'Violet Evergarden: Recollections': 2020,
    'Wave of Cinema: 90\'s Generation': 2021, #Corrected this line
    'Wizzo School': 2023
}

# Impute the release years using the mapping
for movie, release_year in movie_release_mapping.items(): # Iterates through the movie-release mapping
  df.loc[df['Title Name'] == movie, 'Released'] = release_year # Updates the 'Released' column for matching movie titles

# Print the updated DataFrame to verify the changes (optional)
print(df[df['Title Name'].isin(movie_release_mapping.keys())]) # Prints rows where 'Title Name' is in the mapping keys

# Check for missing values in the entire DataFrame
missing_values = df.isnull().sum() # Calculates the number of missing values for each column
# Print the results
print("Missing values in the DataFrame:\n", missing_values) # Prints the missing value counts for each column

# Check for missing values in the 'Released' column
missing_released = df['Released'].isnull().sum() # Calculates the number of missing values in the 'Released' column
# Get the rows with missing values in 'Released'
missing_released_rows = df[df['Released'].isnull()] # Filters the DataFrame for rows with missing 'Released' values
# Print the rows
print("Rows with missing 'Released' values:") # Prints a header
print(missing_released_rows) # Prints the rows with missing 'Released' values

# Convert 'Released' to numeric and handle potential errors
df['Released'] = pd.to_numeric(df['Released'], errors='coerce')

# Calculate the median
median_release_year = df['Released'].median()

# Fill missing values with median
df['Released'] = df['Released'].fillna(median_release_year)

print(df)

movie_release_mapping = {
     'Dusty Slay: Workin/Man  English':2024,
    'LiSA LiVE is Smile Always, Eve&Birth: The Birth at Nippon Budokab': 2022
}

# Check for missing values in the entire DataFrame
missing_values = df.isnull().sum() # Calculates the number of missing values for each column

# Print the results
print("Missing values in the DataFrame:\n", missing_values) # Prints the missing value counts for each column

# Method 1: Fill missing values with the most frequent value (mode)
most_frequent_age_rating = df['Age_Rating'].mode()[0]  # Get the most frequent age rating
df['Age_Rating'].fillna(most_frequent_age_rating, inplace=True)

# Verify the changes
missing_age_ratings_after = df['Age_Rating'].isnull().sum()
print(f"Number of missing values in 'Age_Rating' after imputation: {missing_age_ratings_after}")

# Check for missing values in the entire DataFrame
missing_values = df.isnull().sum() # Calculates the number of missing values for each column
# Print the results
print("Missing values in the DataFrame:\n", missing_values) # Prints the missing value counts for each column

# Define the rating mapping
Age_Rating_mapping = { # Defines a dictionary to map rating labels to numerical values
      'Not Rated' : 0 , 'R' : 1 ,'G' : 2 , 'NR' : 3 , 'PG' : 4 , 'NC-17' : 5 , 'PG-13' : 6 , 'TV-14' : 7 ,
      'TV-G' : 8 ,'TV-MA' : 9 , 'TV-PG' : 10 , 'TV-Y' : 11 , 'TV-Y7' : 12}

# Apply the mapping to the 'Rating' column
df['Age_Rating'] = df['Age_Rating'].map(Age_Rating_mapping) # Maps the 'Rating' column using the rating_mapping dictionary

# Print the 'Rating' column
print("\nAge_Rating column:") # Prints a header
print(df['Age_Rating']) # Prints the 'Rating' column

print(df)

Language_mapping = {
    'Afrikaans': 1, 'Albanian': 2, 'Arabic': 3, 'Bengali': 4, 'Bosnian': 5, 'Brazilian Portuguese': 6,
    'Cantonese': 7, 'Catalan': 8, 'Chinese': 9, 'Czech': 10, 'Danish': 11, 'Dari': 12, 'Dutch': 13,
    'English': 14, 'Filipino': 15, 'Finnish': 16, 'French': 17, 'Georgian': 18, 'German': 19, 'Hindi': 20,
    'Hungarian': 21, 'Icelandic': 22, 'Indonesian': 23, 'Italian': 24, 'Japanese': 25, 'Kannada': 26,
    'Khmer': 27, 'Korean': 28, 'Malayalam': 29, 'Mandarin': 30, 'Marathi': 31, 'Navajo': 32, 'Norwegian': 33,
    'Pashto': 34, 'Persian': 35, 'Polish': 36, 'Portuguese': 37, 'Punjabi': 38, 'Romanian': 39, 'Russian': 40,
    'Serbian': 41, 'Spanish': 42, 'Swahili': 43, 'Swedish': 44, 'Swiss German': 45, 'Tagalog': 46, 'Tamil': 47,
    'Telugu': 48, 'Thai': 49, 'Turkish': 50, 'Ukrainian': 51, 'Urdu': 52, 'Vietnamese': 53, 'Welsh': 54,
    'Yiddish': 55, 'Zulu': 56
}

# Apply the mapping to the 'Rating' column
df['Language'] = df['Language'].map(Language_mapping)

# Print the 'Rating' column
print("\nLanguage column:") # Prints a header
print(df['Language']) # Prints the 'Rating' column

print(df)

# Calculate the difference between the current median and the desired median (2021)
median_diff = 2021 - df['Released'].median()

# Adjust the 'Released' column by adding the difference
df['Released'] = df['Released'] + median_diff

for column in ['Released', 'IMDb']: # Iterates through 'Released' and 'IMDb' columns
# Convert the column to numeric, coercing errors to NaN
  df[column] = pd.to_numeric(df[column], errors='coerce') # Converts the column to numeric type

# Calculate quartiles and IQR
Q1 = df[column].quantile(0.25)
Q3 = df[column].quantile(0.75)
IQR = Q3 - Q1

# Calculate lower and upper bounds
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Cap outliers
df[column] = np.clip(df[column], lower_bound, upper_bound)

# Optionally, print information about outliers
outliers = df[(df[column] == lower_bound) | (df[column] == upper_bound)]
print(f"\nOutliers in {column}:")
print(outliers)

# Create boxplots for 'Released' and 'IMDb'
for column in ['Released', 'IMDb']:
    plt.figure(figsize=(8, 6))  # Adjust figure size as needed
    sns.boxplot(x=df[column])
    plt.title(f'Boxplot of {column}')
    plt.show()

print(df)

# Drop NaNs if any
df = df.dropna(subset=['Released'])

# Calculate the median of 'Released'
median_released = df['Released'].median()

# Identify outliers in 'Released' using IQR with adjusted multiplier
Q1 = df['Released'].quantile(0.25)
Q3 = df['Released'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR  # Standard multiplier for outlier detection
upper_bound = Q3 + 1.5 * IQR

# Replace outliers with the median
df.loc[(df['Released'] < lower_bound) | (df['Released'] > upper_bound), 'Released'] = median_released

# Create boxplots for 'Released' and 'IMDb'
for column in ['Released', 'IMDb']:
    plt.figure(figsize=(8, 6))  # Adjust figure size as needed
    sns.boxplot(x=df[column])
    plt.title(f'Boxplot of {column}')
    plt.show()

"""#2.1 Normalize Numerical Features"""

# Select numerical features for normalization
numerical_features = ['Released', 'IMDb']  # Replace with your actual numerical feature names

# Create a MinMaxScaler object
scaler = MinMaxScaler()

# Apply Min-Max scaling to the selected features
df[numerical_features] = scaler.fit_transform(df[numerical_features])

# Print the DataFrame to verify the changes (optional)
print(df)

# Select numerical features for normalization (e.g., 'Released', 'IMDb')
numerical_features = ['Released', 'IMDb'] # Defines the numerical features to be normalized

# Apply Min-Max scaling
scaler = MinMaxScaler() # Creates a MinMaxScaler object
df[numerical_features] = scaler.fit_transform(df[numerical_features]) # Applies Min-Max scaling to the selected features

# Print after normalization
print("\nDataFrame after numerical feature normalization:") # Prints a header
print(df) # Prints the DataFrame after normalization

df = df.rename(columns={"ï»¿User_ID": "User_ID"})

print(df)

"""#Phase 3

model training
"""

# 1. Train-Test Split
# Assuming 'IMDb' is your target variable:
# Replace problematic column names with actual column names from your DataFrame
X = df.drop(['IMDb', 'Title Name', 'User_ID', 'Language', 'Age_Rating'], axis=1)  # Corrected column names
y = df['IMDb']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # 80/20 split

"""Linear Regression model"""

lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

"""Random Forest Model"""

rf_model = RandomForestRegressor(n_estimators=100, max_depth=None, random_state=42)
rf_model.fit(X_train, y_train)

"""XGBoost Model"""

xgb_model = xgb.XGBRegressor(objective="reg:squarederror", n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)
xgb_model.fit(X_train, y_train)

"""LightGBM model"""

model = lgb.LGBMRegressor(
    objective='regression',
    learning_rate=0.1,
    max_depth=7,
    num_leaves=31,
    n_estimators=200
)

# Create the early stopping callback
early_stopping = lgb.early_stopping(stopping_rounds=10, verbose=True)

# Fit the model, passing the early stopping callback in the 'callbacks' parameter
# Remove verbose=10 and let early_stopping control verbosity
model.fit(X_train, y_train, eval_set=[(X_test, y_test)], callbacks=[early_stopping])

"""Neural Network Model"""

nn_model = Sequential([
    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(1)  # Output layer for regression
])

nn_model.compile(optimizer='adam', loss='mse', metrics=['mae'])
nn_model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.2, verbose=1)

"""Model Evaluation"""

def evaluate_model(name, y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    return f"{name} -> MSE: {mse:.4f}, R²: {r2:.4f}"

lr_pred = lr_model.predict(X_test)
rf_pred = rf_model.predict(X_test)
xgb_pred = xgb_model.predict(X_test)
nn_pred = nn_model.predict(X_test).flatten()
y_pred = model.predict(X_test)

print(evaluate_model("Linear Regression", y_test, lr_pred))
print(evaluate_model("Random Forest", y_test, rf_pred))
print(evaluate_model("XGBoost", y_test, xgb_pred))
print(evaluate_model("Neural Network", y_test, nn_pred))
print(evaluate_model("Light GBM", y_test, y_pred))

"""model improvment

feature engineering
"""

if 'Released' in df.columns:
    df['Movie Age'] = 2025 - df['Released']

df["Age_Rating_Interaction"] = df["Movie Age"] * df["IMDb"]

"""Train test split"""

# Assuming 'IMDb' is your target variable:
# Replace problematic column names with actual column names from your DataFrame
# Print the columns in your DataFrame to verify the correct names:
print(df.columns)

# Update drop to use the correct column names

# Get a list of columns to keep
columns_to_keep = [col for col in df.columns if col not in ['IMDb', 'Title Name', 'User_id']]

# Select only the columns to keep
X = df[columns_to_keep]

y = df['IMDb']

# Check if X is empty and handle it appropriately
if X.empty:
    raise ValueError("The DataFrame X is empty after dropping columns. Please check your column names.")

# Impute missing values using the mean of each column BEFORE converting to numeric
imputer = SimpleImputer(strategy='mean')
X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns) # keep the columns

# Convert all columns in X to numeric, errors='coerce' will replace non-numeric values with NaN
for col in X.columns:
    X[col] = pd.to_numeric(X[col], errors='coerce')

#The imputer step is removed from here

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # 80/20 split

"""Linear Regression model"""

lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

"""Random Forest model"""

rf = RandomForestRegressor(
    n_estimators=100,      # Reduce trees from default 200+
    max_depth=5,          # Limit depth
    min_samples_split=10,
    min_samples_leaf=5,
    random_state=42
)
rf.fit(X_train, y_train)
rf_preds = rf.predict(X_test)
rf_mse = mean_squared_error(y_test, rf_preds)
rf_r2 = r2_score(y_test, rf_preds)

"""XGBoost model"""

xgb = XGBRegressor(
    n_estimators=50,       # Reduce tree count
    max_depth=4,            # Limit tree depth
    learning_rate=0.01,     # Reduce learning rate
    min_child_weight=3,     # Prevent small splits
    subsample=0.8,          # Only use 80% of the data per tree
    colsample_bytree=0.8,   # Use 80% of features per tree
    random_state=42
)
xgb.fit(X_train, y_train)
xgb_preds = xgb.predict(X_test)
xgb_mse = mean_squared_error(y_test, xgb_preds)
xgb_r2 = r2_score(y_test, xgb_preds)

"""Lightgbm model"""

lgb = LGBMRegressor(
    objective="regression",
    learning_rate=0.05,
    max_depth=6,  # Reduce tree complexity
    num_leaves=31,  # Fewer leaf nodes
    min_gain_to_split=0.01,  # Avoid useless splits
    min_data_in_leaf=10,  # Prevent overfitting
    n_estimators=500,
    verbose=-1  # Suppress warnings
)

lgb.fit(X_train, y_train) # Fit using the new model name

"""Neural Network model"""

nn = MLPRegressor(
    hidden_layer_sizes=(64, 32),  # Fewer neurons
    activation='relu',
    solver='adam',
    alpha=0.001,
    learning_rate='adaptive',
    max_iter=500,
    random_state=42
)
nn.fit(X_train, y_train)
nn_preds = nn.predict(X_test)
nn_mse = mean_squared_error(y_test, nn_preds)
nn_r2 = r2_score(y_test, nn_preds)

"""Results after hyperparameter tuning"""

lr_pred = lr_model.predict(X_test)
rf_preds = rf.predict(X_test)
xgb_preds = xgb.predict(X_test)
nn_preds = nn.predict(X_test).flatten()
lgbm_pred = lgb.predict(X_test)

def evaluate_model(name, y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    return f"{name} -> MSE: {mse:.4f}, R²: {r2:.4f}"

print(evaluate_model("Linear Regression", y_test, lr_pred))
print(evaluate_model("Random Forest", y_test, rf_preds))
print(evaluate_model("XGBoost", y_test, xgb_preds))
print(evaluate_model("Neural Network", y_test, nn_preds))
print(evaluate_model("Light GBM", y_test, lgbm_pred))

"""Cross validation"""

kf = KFold(n_splits=5, shuffle=True, random_state=42)

# ✅ Function to perform cross-validation
def cross_validate_model(model, X, y):
    mse_scores = -cross_val_score(model, X, y, cv=kf, scoring='neg_mean_squared_error')  # Convert negative MSE
    r2_scores = cross_val_score(model, X, y, cv=kf, scoring='r2')  # R² score directly
    return np.mean(mse_scores), np.mean(r2_scores)

# ✅ Apply cross-validation to models
models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(n_estimators=100, max_depth=5, min_samples_split=10, min_samples_leaf=5, random_state=42),
    "XGBoost": XGBRegressor(n_estimators=50, max_depth=4, learning_rate=0.01, min_child_weight=3, subsample=0.8, colsample_bytree=0.8, random_state=42),
    "Neural Network": MLPRegressor(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', alpha=0.001, learning_rate='adaptive', max_iter=500, random_state=42),
    "Light GBM": LGBMRegressor(objective="regression", learning_rate=0.05, max_depth=6, num_leaves=31, min_gain_to_split=0.01, min_data_in_leaf=10, n_estimators=500, verbose=-1)
}

# ✅ Use X, y instead of X_train, y_train for proper cross-validation
print("===== Cross-Validation Results =====")
for name, model in models.items():
    mse, r2 = cross_validate_model(model, X, y)
    print(f"{name} -> MSE: {mse:.4f}, R²: {r2:.4f}")

"""#Phase 4"""

#Histogram of IMDb ratings
plt.figure(figsize=(8,5))
sns.histplot(df['IMDb'], bins = 20, kde = True)
plt.title("Distribution of IMDb Ratings")
plt.xlabel("IMDb Rating")
plt.ylabel("Frequency")
plt.show()

# Check distributions of features (normal vs. skewed)
# Set style
sns.set(style="whitegrid")

# Select numerical features
df_numeric = df.select_dtypes(include=['number'])

# Create subplots
fig, axes = plt.subplots(nrows=len(df_numeric.columns), ncols=1, figsize=(8, 4 * len(df_numeric.columns)))

# Plot histograms with KDE and calculate skewness
skew_values = {}
for i, feature in enumerate(df_numeric.columns):
    sns.histplot(df_numeric[feature], bins=30, kde=True, ax=axes[i])
    skewness = skew(df_numeric[feature].dropna())  # Remove NaN values before computing skew
    skew_values[feature] = skewness
    axes[i].set_title(f"Distribution of {feature} (Skewness: {skewness:.2f})")

# Adjust layout
plt.tight_layout()
plt.show()

# Print skewness values
print("Feature Skewness:")
for feature, value in skew_values.items():
    print(f"{feature}: {value:.2f} ( {'Normal' if abs(value) < 0.5 else 'Skewed'} )")

# ✅ Remove special characters from column names
df.columns = df.columns.str.encode('ascii', 'ignore').str.decode('ascii')

# ✅ Filter only numeric columns to avoid errors
numeric_columns = df.select_dtypes(include=['number']).columns

# ✅ Create individual boxplots for each numeric column
for column in numeric_columns:
    plt.figure(figsize=(6, 4))  # New figure for each column
    sns.boxplot(y=df[column])  # Vertical boxplot
    plt.title(f"Boxplot of {column}")
    plt.ylabel("Values")
    plt.grid(True)
    plt.show()

#Scatter plot: Release Year vs IMDb Rating
plt.figure(figsize=(8,5))
sns.scatterplot(x = df['Released'], y = df['IMDb'])
plt.title("Release Year vs IMDb Rating")
plt.xlabel("Release Year")
plt.ylabel("IMDb Rating")
plt.show()

# Compute correlations with the target variable (change "IMDb" to your actual target)
# Select only numerical features for correlation calculation
numerical_df = df.select_dtypes(include=['number'])

# Calculate correlations on the numerical DataFrame
target_corr = numerical_df.corr()["IMDb"].drop("IMDb").sort_values(ascending=False)

# Plot feature correlations with target
plt.figure(figsize=(8, 5))
sns.barplot(x=target_corr.values, y=target_corr.index, hue=target_corr.index, palette="coolwarm", dodge=False, legend=False)  # Set hue and legend=False
plt.xlabel("Correlation with IMDb Rating")
plt.title("Feature Correlation with Target")
plt.show()

# Train Linear Regression model
lr_model = LinearRegression()
lr_model.fit(X, y)

# Get feature importance (absolute coefficient values)
lr_importance = np.abs(lr_model.coef_)

# Create DataFrame for plotting
lr_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': lr_importance}).sort_values(by='Importance', ascending=False)

# Plot
plt.figure(figsize=(8, 6))
sns.barplot(x='Importance', y='Feature', hue='Feature', data=lr_importance_df, palette="Blues_r", dodge=False, legend=False)  # Set hue and legend=False
plt.title("Feature Importance - Linear Regression")
plt.xlabel("Absolute Coefficient Value")
plt.ylabel("Features")
plt.show()

# Define features and target variable
X = df.drop(columns=["IMDb", "Title Name", "User_ID"])  # Drop non-numeric and target
y = df["IMDb"]

# Train a Random Forest model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X, y)

# Get feature importance scores
feature_importance = model.feature_importances_

# Convert to DataFrame for better visualization
import pandas as pd
feat_imp_df = pd.DataFrame({"Feature": X.columns, "Importance": feature_importance})
feat_imp_df = feat_imp_df.sort_values(by="Importance", ascending=False)

# Plot feature importance
plt.figure(figsize=(8, 5))
sns.barplot(x='Importance', y='Feature', hue='Feature', data=feat_imp_df, palette="viridis", dodge=False, legend=False)  # Pass feat_imp_df to data
plt.xlabel("Feature Importance Score", labelpad=0.7)
plt.title("Feature Importance (Random Forest)")
plt.show()

xgb_model = XGBRegressor(n_estimators=100, random_state=42)
xgb_model.fit(X, y)

# Get feature importance from the model
xgb_importance = xgb_model.feature_importances_

# Create DataFrame for plotting
xgb_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': xgb_importance}).sort_values(by='Importance', ascending=False)

# Plot
plt.figure(figsize=(8, 6))
sns.barplot(x='Importance', y='Feature', hue='Feature', data=xgb_importance_df, palette="Oranges_r", dodge=False, legend=False)  # Set hue and legend=False
plt.title("Feature Importance - XGBoost")
plt.xlabel("Feature Importance Score")
plt.ylabel("Features")
plt.show()

#Define Features and Target
X = df[['Released','Age_Rating','Language', 'Movie Age', 'Age_Rating_Interaction']]
y = df['IMDb']

#Option 2: Impute NaN values with the mean of 'y'
y = y.fillna(y.mean())

# Impute missing values in X using SimpleImputer
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')  # or strategy='median'
X = imputer.fit_transform(X)

#Train a Simple Random Forest Model
# model = RandomForestRegressor() # This line is not needed anymore
# model.fit(X,y) # This line is not needed anymore

# Instead of using nn_model (a Keras model), use 'nn' (an sklearn model)
# for permutation importance:
nn = MLPRegressor(hidden_layer_sizes=(64, 32), activation='relu', solver='adam',
                  alpha=0.001, learning_rate='adaptive', max_iter=500, random_state=42)
nn.fit(X, y)  # Fit the model to your data

# Compute permutation importance using the sklearn model 'nn'
nn_importance = permutation_importance(nn, X, y, n_repeats=10, random_state=42)
nn_importance_values = nn_importance.importances_mean

# Create DataFrame for plotting
nn_importance_df = pd.DataFrame({'Feature': ['Released','Age_Rating','Language', 'Movie Age', 'Age_Rating_Interaction'],
                                 'Importance': nn_importance_values}).sort_values(by='Importance', ascending=False)

# Plot
plt.figure(figsize=(8, 6))
sns.barplot(x='Importance', y='Feature', hue='Feature', data=nn_importance_df, palette="Purples_r", dodge=False, legend=False)  # Set hue and legend=False
plt.title("Feature Importance - Neural Network")
plt.xlabel("Permutation Importance Score")
plt.ylabel("Features")
plt.show()

#Define Features and Target
X = df[['Released','Age_Rating','Language', 'Movie Age', 'Age_Rating_Interaction']]
y = df['IMDb']

#Option 2: Impute NaN values with the mean of 'y'
y = y.fillna(y.mean())

# Impute missing values in X using SimpleImputer
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')  # or strategy='median'
X = imputer.fit_transform(X)

# Convert the NumPy array back to a Pandas DataFrame
X = pd.DataFrame(X, columns=['Released','Age_Rating','Language', 'Movie Age', 'Age_Rating_Interaction']) # Re-assign column names

#Train LightGBM Model with the updated X
lgb = LGBMRegressor(objective="regression", learning_rate=0.05, max_depth=6, num_leaves=31,
                    min_gain_to_split=0.01, min_data_in_leaf=10, n_estimators=500, verbose=-1)
lgb.fit(X, y)  # Fit using the new model name and updated data


# Get feature importances after retraining
lgb_importance = lgb.feature_importances_

# Create DataFrame for plotting
lgb_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': lgb_importance}).sort_values(by='Importance', ascending=False)

# Plot
plt.figure(figsize=(8, 6))
sns.barplot(x='Importance', y='Feature', hue='Feature', data=lgb_importance_df, palette="Blues_r", dodge=False, legend=False)
plt.title("Feature Importance - LightGBM")
plt.xlabel("Feature Importance Score")
plt.ylabel("Features")
plt.show()

# ✅ Create DataFrame for feature importances
# Get the common features across all models
common_features = list(set(X.columns) & set(feature_importance.index) & set(nn_importance_df['Feature']) & set(xgb_importance_df['Feature']) & set(lgb_importance_df['Feature']))

# Filter the importance arrays to include only common features
lr_importance_filtered = [lr_importance[X.columns.get_loc(f)] for f in common_features if f in X.columns]
nn_importance_values_filtered = [nn_importance_values[nn_importance_df['Feature'].tolist().index(f)] for f in common_features if f in nn_importance_df['Feature'].tolist()]
feature_importance_filtered = [feature_importance[feature_importance.index.get_loc(f)] for f in common_features if f in feature_importance.index]
xgb_importance_filtered = [xgb_importance[xgb_importance_df['Feature'].tolist().index(f)] for f in common_features if f in xgb_importance_df['Feature'].tolist()]
lgb_importance_filtered = [lgb_importance[lgb_importance_df['Feature'].tolist().index(f)] for f in common_features if f in lgb_importance_df['Feature'].tolist()]

# Create DataFrame
feature_importance_df = pd.DataFrame({
    "Feature": common_features,
    "Linear Regression": lr_importance_filtered,
    "Neural Network": nn_importance_values_filtered,
    "Random Forest": feature_importance_filtered,
    "XGBoost": xgb_importance_filtered,
    "LightGBM": lgb_importance_filtered
}).set_index("Feature")

# Normalize feature importance (optional, if values have large scale differences)
feature_importance_df = feature_importance_df / feature_importance_df.max()

# Plot all feature importances together
feature_importance_df.plot(kind="barh", figsize=(12, 8), colormap="viridis", alpha=0.8)

plt.xlabel("Normalized Feature Importance Score")
plt.ylabel("Features")
plt.title("Comparison of Feature Importance Across Models")
plt.legend(title="Models", fontsize=10)
plt.grid(axis="x", linestyle="--", alpha=0.7)

plt.show()

#Heatmap of Feature Correlations
plt.figure(figsize=(10,8))
# Select only numerical features for correlation calculation
numerical_df = df.select_dtypes(include=['number'])
sns.heatmap(numerical_df.corr(), annot=True, cmap='coolwarm')
plt.title("Feature Correlations Heatmap")
plt.show()

# ✅ Define models
models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42),
    "XGBoost": XGBRegressor(n_estimators=50, max_depth=4, learning_rate=0.01, random_state=42),
    "Neural Network": MLPRegressor(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=500, random_state=42),
    "LightGBM": LGBMRegressor(objective="regression", learning_rate=0.05, max_depth=6, num_leaves=31, n_estimators=500, verbose=-1)
}

# ✅ Train models & make predictions
plt.figure(figsize=(15, 10))
for i, (name, model) in enumerate(models.items()):
    model.fit(X_train, y_train)  # Train the model
    y_pred = model.predict(X_test)  # Predict on test data

    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # ✅ Scatter plot
    plt.subplot(2, 3, i+1)  # Create subplot grid (2 rows, 3 columns)
    sns.scatterplot(x=y_test, y=y_pred, alpha=0.7)  # Scatter plot
    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r', linestyle="--")  # Perfect fit line
    plt.xlabel("Actual IMDb Rating")
    plt.ylabel("Predicted IMDb Rating")
    plt.title(f"{name}\nMSE: {mse:.4f}, R²: {r2:.4f}")

plt.tight_layout()
plt.show()

X_pred = df[X_train.columns].reindex(columns=X_train.columns)

models = {
    "Linear Regression": lr_model.predict(X_pred),
    "Random Forest": model.predict(X_pred[X.columns]),  # Select columns used to train 'model'
    "XGBoost": xgb_model.predict(X_pred[X.columns]),  # Select columns used to train 'xgb_model'
    "Neural Network": nn_model.predict(X_pred[X.columns]),  # Select columns used to train 'nn_model'
    "LightGBM": lgb.predict(X_pred[X.columns])  # Select columns used to train 'lgb'
}

plt.figure(figsize=(12, 8))

# Scatter plot for each model
for name, y_pred in models.items():
    sns.scatterplot(x=y, y=y_pred, label=name, alpha=0.6)

# Perfect prediction line
plt.plot([y.min(), y.max()], [y.min(), y.max()], color='red', linestyle='--')

plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Actual vs. Predicted Values for Different Models")
plt.legend(title="Models")
plt.grid(True)

plt.show()

# ✅ Define models
models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42),
    "XGBoost": XGBRegressor(n_estimators=50, max_depth=4, learning_rate=0.01, random_state=42),
    "Neural Network": MLPRegressor(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=500, random_state=42),
    "LightGBM": LGBMRegressor(objective="regression", learning_rate=0.05, max_depth=6, num_leaves=31, n_estimators=500, verbose=-1)
}

# ✅ Train models & get predictions
predictions = {}  # Store predictions for each model
for name, model in models.items():
    model.fit(X_train, y_train)
    # Ensure X has the same columns as X_train and in the same order
    predictions[name] = model.predict(X[X_train.columns])  # Predict using X with X_train's columns

# ✅ Compute residuals and create DataFrame
residuals_df = pd.DataFrame({name: y - predictions[name] for name in models})

# Box plot of residuals
plt.figure(figsize=(10, 6))
sns.boxplot(data=residuals_df, palette="coolwarm")

plt.xlabel("Models")
plt.ylabel("Residuals (Errors)")
plt.title("Comparison of Residual Distributions Across Models")
plt.grid(True)

plt.show()

# ✅ Define models
models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42),
    "XGBoost": XGBRegressor(n_estimators=50, max_depth=4, learning_rate=0.01, random_state=42),
    "Neural Network": MLPRegressor(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', max_iter=500, random_state=42),
    "LightGBM": LGBMRegressor(objective="regression", learning_rate=0.05, max_depth=6, num_leaves=31, n_estimators=500, verbose=-1)
}

# ✅ Train models & get predictions
predictions = {}  # Store predictions for each model
for name, model in models.items():
    model.fit(X_train, y_train)
    # Ensure X has the same columns as X_train and in the same order
    predictions[name] = model.predict(X[X_train.columns])  # Predict using X with X_train's columns


plt.figure(figsize=(12, 8))

# Scatter plot for residuals
for name, y_pred in predictions.items(): #use predictions instead of models
    residuals = y - y_pred
    sns.scatterplot(x=y_pred, y=residuals, label=name, alpha=0.6)

# Reference line at zero residuals
plt.axhline(y=0, color='red', linestyle='--')

plt.xlabel("Predicted Values")
plt.ylabel("Residuals (Errors)")
plt.title("Residual Plot: Predicted Values vs. Errors for Different Models")
plt.legend(title="Models")
plt.grid(True)

plt.show()

# Compute residuals for each model
# Change models.items() to predictions.items() to access predicted values
residuals_df = pd.DataFrame({name: y - predictions[name] for name in predictions})

# Box plot of residuals
plt.figure(figsize=(10, 6))
sns.boxplot(data=residuals_df, palette="coolwarm")

plt.xlabel("Models")
plt.ylabel("Residuals (Errors)")
plt.title("Comparison of Residual Distributions Across Models")
plt.grid(True)

plt.show()

plt.figure(figsize=(12, 8))

# Plot residuals for each model
for name, model in models.items():
    # Get predictions using the model with the correct columns
    # Reindex X to match the columns of X_train
    X_pred = X[X_train.columns].reindex(columns=X_train.columns)
    y_pred = model.predict(X_pred)

    residuals = y - y_pred  # Calculate residuals
    sns.scatterplot(x=y_pred, y=residuals, label=name, alpha=0.6)

# Add reference line at zero
plt.axhline(y=0, color='red', linestyle='--')

plt.xlabel("Predicted Values")
plt.ylabel("Residuals (Errors)")
plt.title("Residuals vs. Predicted Values for Different Models")
plt.legend(title="Models")
plt.grid(True)

plt.show()